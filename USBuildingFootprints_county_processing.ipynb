{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating County Files from the Microsoft US Building Footprints Dataset\n",
    "\n",
    "\n",
    "Microsoft recently made freely available a dataset of [building footprints](https://github.com/Microsoft/USBuildingFootprints) for the entire USA. Thank you Microsoft! These footprints were generated using machine learning on satellite imagery. Although not perfect - buildings may be missing or imperfectly represented - this is a unique and valuable dataset that attempts to map every building in the US. Itâ€™s also really cool to visualize and explore. You can read more about the data on the [Bing Blog](https://blogs.bing.com/maps/2018-06/microsoft-releases-125-million-building-footprints-in-the-us-as-open-data) and in this [New York Times article](https://www.nytimes.com/interactive/2018/10/12/us/map-of-every-building-in-the-united-states.html), the latter of which includes some great maps of the data. \n",
    "\n",
    "On the [Github site](https://github.com/Microsoft/USBuildingFootprints) the data are made available as [GeoJSON](https://geojson.org) files, one per state. For some states like CA, FL, IL, MI, NY, PA, TX, OH, and a few others, these files are huge - 1 GB or larger.  These large files are extremely difficult to work with on your average personal computer, whether with desktop software like ArcGIS or QGIS or programmatically in R or Python. Moreover, many folks who work with geospatial data have more experience working with ESRI Shapefiles rather than GeoJSON data.\n",
    "\n",
    "Consequently, there are a number of web posts discussing the desire for and ways to wrangle these data into smaller county based files. This is also a task that I was asked to help out with to make these data more widely available to the UC Berkeley campus community. \n",
    "\n",
    "Unfortnately, none of the suggested approaches for splitting the data into county files worked for me. Consequently, I took advantage of some time off due to a broken ankle and came up with an approach that leverages [PostGIS](https://postgis.net/), the geospatial extension to the fantastic free and open source database software [PostgreSQL](https://www.postgresql.org/). \n",
    "\n",
    "What I liked about this approach is that it gave me an opportunity to get reacquainted with PostGIS, a tool that I used all the time in my previous position but haven't used for a while. It's a great tool for scaling up geospatial data management and analysis. As a new twist, I brought Python to the party and interacted with PostGIS using the [psycopg2](http://initd.org/psycopg/docs/) database adapter.\n",
    "\n",
    "My steps are detailed in this notebook. It ain't pretty and nor is it perfect but it works! That said, this is a draft and improvements to the process are sure to be needed. You can check out the processing results for California counties in this [Google drive folder](https://drive.google.com/drive/folders/1-XGvS25tQKKQ3HTqWjAfLJ4PaeXJ9yyY?usp=sharing).  \n",
    "\n",
    "Given that this is the holiday season, I call it the **ugly sweater** of workflows. Feel free to send me an email if you have suggested improvements for any of the steps!\n",
    "- Patty Frontiera, [D-Lab](https://dlab.berkeley.edu), UC Berkeley (pattyf@berkeley.edu)\n",
    "\n",
    "<img width=\"600px\" src=\"./ugly_sweater.png\"></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries we will use\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Census TIGER/Line data for US Counties\n",
    "\n",
    "Download the TIGER/Line boundary file for all US Counties from the Census website. \n",
    "\n",
    "Do not use the Census Cartographic Boundary files as you may miss some footprints at boundaries of states / census tracts.\n",
    "\n",
    "### Loading County TIGER/Line Shapefile into Postgis\n",
    "This step was done manually. Could automate in this notebook. The steps were:\n",
    "\n",
    "Things to do to make `counties_sub` table:\n",
    "- Download the US County TIGER/Line shapefile (tl_2018_us_county.shp) from the [Census Website](https://www.census.gov/geo/maps-data/data/tiger.html\n",
    "- Open the shapefile in QGIS or ArcGIS to check it and then export to a Shapefile or GeoJSON file, setting the output CRS to WGS84 (by default census data are NAD83).\n",
    "- Use pgsql2shp or ogr2ogr to load the county shapefile/GeoJSON file into postgis.\n",
    "- Explicitly set the CRS of the counties table in PostGIS to 4326\n",
    "- Create a GIST (geo index) on the counties file\n",
    "- Use St_subdivide to make a new table `counties_sub` that will be used for more efficient spatial intersection queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Read in the County Boundary Data as a Geopandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uscounties = gpd.read_file(\"data/tl_2018_us_county/tl_2018_us_county.shp\")\n",
    "uscounties = uscounties[[\"STATEFP\",\"COUNTYFP\",\"GEOID\",\"NAME\",\"geometry\"]]\n",
    "\n",
    "uscounties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary Stats on the counties\n",
    "print(\"The number of counties in the US Counties file is: \", len(uscounties.index))\n",
    "print(\"The number of distinct states in the US Counties File is: \", len(set(uscounties.STATEFP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "If the number of states (plus DC) in the US Counties file is greater than **51** then the file includes the counties or county equivalents in US Territories. IF the Microsoft Buildgings Footprints does not contain data for these places then we should remove those rows from the uscounties geopandas dataframe.\n",
    "\n",
    "<blockquote>\n",
    "The United States of America is a federal republic consisting of 50 states, a federal district (Washington, D.C., the capital city of the United States), five major territories, and various minor islands. - Wikipedia\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Read in a table of all the State Building Footprints you want to process.\n",
    "\n",
    "This table was created from the table on the [Microsoft Building footprints Github site](https://github.com/Microsoft/USBuildingFootprints) and imported into [this Google Sheet](https://docs.google.com/spreadsheets/d/1wEk1DC-B4AjKdAUJ-9QtcTv7IIPlTvEhuX2HV2o01SI/edit?usp=sharing). You can download it as a CSV file to use with this script.\n",
    "\n",
    "- If you don't want to process the entire USA, delete any states you don't want to process from the CSV file or delete the rows from the dataframe after you import.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfips_lookup = pd.read_csv(\"data/USA_statefips_lookup.csv\", dtype={\"STATE\": str, \"FIPS\": str, \"ABBREV\": str, 'BLDG_GEOJSON': str})\n",
    "sfips_lookup.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chek the number of states/geojson files to be processed.\n",
    "print(\"The Microsoft BuildingFootprints dataset includes footprints for \", len(sfips_lookup.index), \" states plus DC.\")\n",
    "print(\"The total number of footprints in the dataset is: \", sum(sfips_lookup['BLDG_COUNT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sfips_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Download & unzip all state bldg footprint files from\n",
    "\n",
    "- Go to https://github.com/Microsoft/USBuildingFootprints, find the links to the state GeoJSON footprint files and download the ones you want to process - manually or via a script, curl, wget etc.  \n",
    "\n",
    "- Once they are downloaded you can execute a simple command in the folder in which the downloaded files reside to unzip them all:\n",
    "\n",
    "<pre>\n",
    "for file in *.zip; do unzip $file; done\n",
    "</pre>\n",
    "\n",
    "**We could do this in the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Load the GeoJSON files into PostGIS\n",
    "\n",
    "- Use the code below to create a **shell script** (`load_geojson_ogr.sh`) to load all unzipped geojson files into PostGIS database tables.\n",
    "\n",
    "- If you are not processing all states, subset the `sfips_lookup` dataframe.\n",
    "    \n",
    "- Change the permissions on the shell script file (`chmod 755 load_geojson_ogr.sh`) to make it executable.\n",
    "- Run the script (./load_geojson_ogr.sh) from the directory in which the geojson files reside\n",
    "\n",
    "**Suggested update not used by this process - read in only polygon geometry by adding:** -where \"OGR_GEOMETRY='POLYGON or MULTI'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('load_geojson_ogr.sh', 'w')\n",
    "\n",
    "for index, row in sfips_lookup.iterrows():\n",
    "    jfile =row[\"BLDG_GEOJSON\"]\n",
    "    scode = row['ABBREV']\n",
    "    tname = scode + \"_footprints\"\n",
    "    #print(scode, \":\", jfile , \": \", tname)\n",
    "    f.write('ogr2ogr -f \"PostgreSQL\" PG:\"dbname=pattyf user=pattyf\" ' + jfile + ' -nln ' + tname +  ' -a_srs \"EPSG:4326\"\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you run the script `./load_geojson_ogr.sh` ?\n",
    "\n",
    "This is likely the longest part of the process, takes ~ 5-10 hours I estimate.\n",
    "\n",
    "- I started it around 9pm and at 12:30am still 24 states to go (Nevada - Wyoming).\n",
    "\n",
    "- You may want to split this into 2-4 scripts - my computer is starting to overheat...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Create a function to Subdivide each State Footprints table.\n",
    "\n",
    "Create a function that will use the PostGIS function `ST_Subdivide` to subdivide each of the state footprints tables to improve the performance of spatial intersection queries. \n",
    "\n",
    "This step was informed by this [Paul Ramsey blog post](http://blog.cleverelephant.ca/2017/12/postgis-scaling.html).\n",
    "\n",
    "Here are the steps that will be needed, with an example using Rhode Island (RI) data:\n",
    "\n",
    "<pre>\n",
    "-- Create a new subdivided table\n",
    "CREATE TABLE ri_footprints_sub AS\n",
    "SELECT ogc_fid, ST_Subdivide(wkb_geometry) AS geom\n",
    "FROM ri_footprints;\n",
    "\n",
    "-- Update the SRID on each new sub table\n",
    "select UpdateGeometrySRID('ri_footprints_sub','geom',4326);\n",
    "\n",
    "--Create the GIST on each new sub table\n",
    "CREATE INDEX ri_footprints_sub_geom_idx ON ri_footprints_sub USING GIST ( geom );\n",
    "</pre>\n",
    "\n",
    "\n",
    "The function to automate the first step is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the function within postgresql to create a sub table\n",
    "CREATE OR REPLACE FUNCTION create_state_sub_table(tsub_name varchar(30), t_name varchar(30))\n",
    "  RETURNS VOID AS\n",
    "$func$\n",
    "BEGIN\n",
    "\n",
    "EXECUTE format('\n",
    "        CREATE TABLE IF NOT EXISTS %I AS\n",
    "            SELECT ogc_fid, ST_Subdivide(wkb_geometry) AS geom\n",
    "            FROM %I;', tsub_name, t_name);\n",
    "\n",
    "END\n",
    "$func$ LANGUAGE plpgsql;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Implement the function over all states.\n",
    "\n",
    "For example:\n",
    "<pre>\n",
    "create_state_sub_table('al_footprints_sub', 'al_footprints')\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement function create_state_sub_table over all state footprints tables.\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='pattyf' user='pattyf' host='localhost' password='XXXXXXX'\")\n",
    "    print(\"connected\")\n",
    "\n",
    "    for index, row in sfips_lookup.iterrows():\n",
    "        scode = row['ABBREV'].lower()\n",
    "        sub_table_name = scode + \"_footprints_sub\"   \n",
    "        table_name = scode + \"_footprints\"\n",
    " \n",
    "        #create_state_sub_table(sub_table_name, table_name)\n",
    "        #create_state_sub_table('al_footprints_sub', 'al_footprints')\n",
    "        q1 = \"\"\"SELECT create_state_sub_table('{sname}', '{tname}');\"\"\".format(sname=sub_table_name, tname=table_name)\n",
    "\n",
    "        print(q1)\n",
    "\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            print('about to execute query')\n",
    "            cur.execute(q1)\n",
    "            print('about to commit query results')\n",
    "            conn.commit()\n",
    "            # close the database communication\n",
    "            cur.close()\n",
    "\n",
    "        except psycopg2.DatabaseError as error:\n",
    "            print(error)\n",
    "\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "\n",
    "print(\"Back from Postgis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Set the SRID on all the sub tables\n",
    "\n",
    "For example:\n",
    "\n",
    "<pre>\n",
    "-- Update the SRID\n",
    "select UpdateGeometrySRID('ri_footprints_sub','geom',4326);\n",
    "</pre>\n",
    "\n",
    "*Could add this to the `create_state_sub_table` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upate SRID on all sub tables\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='pattyf' user='pattyf' host='localhost' password='XXXXXX'\")\n",
    "    print(\"connected\")\n",
    "\n",
    "    for index, row in sfips_lookup.iterrows():\n",
    "        scode = row['ABBREV'].lower()\n",
    "        sub_table_name = scode + \"_footprints_sub\"# Update the SRID on the sub table    \n",
    "        \n",
    "        # select UpdateGeometrySRID('ri_footprints_sub','geom',4326);\n",
    "        q1 = \"\"\"select UpdateGeometrySRID('{name}','geom',4326);\"\"\".format(name=sub_table_name)\n",
    "\n",
    "        print(q1)\n",
    "\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            print('about to execute query')\n",
    "            cur.execute(q1)\n",
    "            print('about to commit query results')\n",
    "            conn.commit()\n",
    "            # close the database communication\n",
    "            cur.close()\n",
    "\n",
    "        except psycopg2.DatabaseError as error:\n",
    "            print(error)\n",
    "\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "\n",
    "print(\"Back from Postgis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. Create the GIST on each of the sub table\n",
    "\n",
    "For example:\n",
    "<pre>\n",
    "SELECT CREATE INDEX ri_footprints_sub_geom_idx ON ri_footprints_sub USING GIST ( geom );\n",
    "\n",
    "</pre>\n",
    "\n",
    "\n",
    "This step takes about 1 - 4 minutes per state, depending on table size.\n",
    "\n",
    "*Could add this to the `create_state_sub_table` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create GIST on all sub tables\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='pattyf' user='pattyf' host='localhost' password='XXXXXXX'\")\n",
    "    print(\"connected\")\n",
    "\n",
    "    for index, row in sfips_lookup.iterrows():\n",
    "        scode = row['ABBREV'].lower()\n",
    "        subtable_name = scode + \"_footprints_sub\"# Update the SRID on the sub table \n",
    "        subtable_index = subtable_name +  '_geom_idx'\n",
    "        \n",
    "        # select UpdateGeometrySRID('ri_footprints_sub','geom',4326);\n",
    "        q1 = \"\"\"CREATE INDEX {stable_index} on {stable} USING GIST (geom);\"\"\".format(stable_index = subtable_index, stable=subtable_name)\n",
    "\n",
    "        print(q1)\n",
    "\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            print('about to execute query')\n",
    "            cur.execute(q1)\n",
    "            print('about to commit query results')\n",
    "            conn.commit()\n",
    "            # close the database communication\n",
    "            cur.close()\n",
    "\n",
    "        except psycopg2.DatabaseError as error:\n",
    "            print(error)\n",
    "\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "\n",
    "print(\"Back from Postgis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11. - OPTIONAL. Make sure all our tables have valid geometry\n",
    "\n",
    "The command to do this looks like this:\n",
    "\n",
    "<pre>\n",
    "UPDATE counties_sub SET geom = ST_MakeValid(geom) WHERE NOT ST_IsValid(geom);\n",
    "</pre>\n",
    "\n",
    "*This step can take a several hours ~ 5!*\n",
    "\n",
    "**NOTE** Try without this step now that we are doing point in polygon intersection rather than polygon-polygon (which was throughing errors that this didn't resolve anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure all sub tables have valid geometry\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='pattyf' user='pattyf' host='localhost' password='XXXXXXX'\")\n",
    "    print(\"connected\")\n",
    "\n",
    "    for index, row in sfips_lookup.iterrows():\n",
    "        scode = row['ABBREV'].lower()\n",
    "        subtable_name = scode + \"_footprints_sub\"# Update the SRID on the sub table \n",
    "        \n",
    "        # select UpdateGeometrySRID('ri_footprints_sub','geom',4326);\n",
    "        q1 = \"\"\"UPDATE {stable} SET geom = ST_MakeValid(geom) WHERE NOT ST_IsValid(geom);\"\"\".format(stable=subtable_name)\n",
    "\n",
    "        print(q1)\n",
    "\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            print('about to execute query')\n",
    "            cur.execute(q1)\n",
    "            print('about to commit query results')\n",
    "            conn.commit()\n",
    "            # close the database communication\n",
    "            cur.close()\n",
    "\n",
    "        except psycopg2.DatabaseError as error:\n",
    "            print(error)\n",
    "\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "\n",
    "print(\"Back from Postgis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12. Create function to add centroid geometry to all county tables.\n",
    "\n",
    "Create function to add centroid geometry to each state footprints subdivided table. This is an indexing operation that will speed up spatial intersections.\n",
    "\n",
    "What this step means is as follows: a county footprints table will contain the footprints for all state footprint centroids that intersect the county boundary polygon.\n",
    "\n",
    "#### Cautions:\n",
    "\n",
    "- Footprints in a county file whose centroid is not in the county could be dropped by this process if the county borders another state (because we are only querying against the state footprints). If it borders a county in the same state the footprint would be reallocated to the other county (which is not necessarily a problem).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the function within postgresql to create a sub table\n",
    "CREATE OR REPLACE FUNCTION create_state_centroids(tname varchar(30), gindex_name varchar(64))\n",
    "  RETURNS VOID AS\n",
    "$func$\n",
    "BEGIN\n",
    "\n",
    "-- Create a new geometry column to hold the centroid_geom\n",
    "EXECUTE format('\n",
    "        SELECT AddGeometryColumn (''public'', ''%I'', ''centroid_geom'', 4326, ''POINT'', 2);', tname);\n",
    "\n",
    "-- Populate the centroid_geom column\n",
    "EXECUTE format('\n",
    "        UPDATE %I SET centroid_geom = ST_Centroid(geom);', tname);\n",
    "               \n",
    "--Create the index on the centroid_geom\n",
    "EXECUTE format('\n",
    "        CREATE INDEX %I ON %I USING GIST (centroid_geom);', gindex_name, tname );\n",
    "\n",
    "END\n",
    "$func$ LANGUAGE plpgsql;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13.  Apply the create_state_centroids function\n",
    "\n",
    "WARNING: This took about 5 hours for all 50 states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Populate centroid_geom\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='pattyf' user='pattyf' host='localhost' password='XXXXXXX'\")\n",
    "    print(\"connected\")\n",
    "\n",
    "    for index, row in sfips_lookup.iterrows():\n",
    "        scode = row['ABBREV'].lower()\n",
    "        subtable_name = scode + \"_footprints_sub\"# Update the SRID on the sub table \n",
    "        subtable_index = scode + \"_footprints_sub_centroid_geom_idx\"# Update the SRID on the sub table\n",
    "        \n",
    "        # The Query to update the tables with centroid geom\n",
    "        q1 = \"\"\"SELECT create_state_centroids('{sname}', '{gindex}');\"\"\".format(sname=subtable_name, gindex=subtable_index)\n",
    "\n",
    "        print(q1)\n",
    "\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            print('about to execute query')\n",
    "            cur.execute(q1)\n",
    "            print('about to commit query results')\n",
    "            conn.commit()\n",
    "            # close the database communication\n",
    "            cur.close()\n",
    "\n",
    "        except psycopg2.DatabaseError as error:\n",
    "            print(error)\n",
    "\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "\n",
    "print(\"Back from Postgis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14. Create the County Footprint Tables\n",
    "\n",
    "Spatial intersection queries with the sub tables.\n",
    "\n",
    "**This took about 40 minutes for all counties in all states.** Fast because we first subdivided the data.\n",
    "\n",
    "The query returns building footprint polygons whose centroid intersects the boundary or interior of a county polygon.\n",
    "\n",
    "The output will be one table for each county in each state for entire USA!\n",
    "\n",
    "---\n",
    "\n",
    "### The query looks like this:\n",
    "<pre>\n",
    "CREATE TABLE IF NOT EXISTS {county_tname} as\n",
    "SELECT\n",
    "  c.name,\n",
    "  f.geom AS the_geom\n",
    "FROM\n",
    "    counties_sub c,\n",
    "    {state_footprints_sub} f\n",
    "WHERE\n",
    "    c.geoid = '{geoid}' AND\n",
    "    ST_Intersects(c.geom, f.centroid_geom);\"\"\".format(county_tname=ctable_name, state_footprints_sub=subtable_name, geoid=the_geoid)\n",
    "\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create County footprints tables\n",
    "# This took 2 hours?\n",
    "# Data Frame to keep track of processing notes\n",
    "proc_errors_df = pd.DataFrame(columns = ['state', 'county', 'geoid','error'])\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='pattyf' user='pattyf' host='localhost' password='XXXXXXX'\")\n",
    "    print(\"connected\")\n",
    "\n",
    "    for index, row in sfips_lookup.iterrows():\n",
    "        scode = row['ABBREV'].lower()\n",
    "        sfips = row['FIPS']\n",
    "        subtable_name = scode + \"_footprints_sub\" \n",
    "        \n",
    "        print(\"Processing : \", scode, \" With FIPS: \", sfips)\n",
    "        state_county_df = uscounties[uscounties['STATEFP']==sfips]\n",
    "        \n",
    "        for item, row in state_county_df.iterrows():\n",
    "            the_geoid = row['GEOID']\n",
    "            the_county = row['NAME']\n",
    "            ctable_name = scode + \"_\" + the_geoid + \"_footprints\"\n",
    "            print(\"About to create table: \", ctable_name)\n",
    "            \n",
    "            q1 = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {county_tname} as\n",
    "SELECT\n",
    "  c.name,\n",
    "  f.geom AS the_geom\n",
    "FROM\n",
    "    counties_sub c,\n",
    "    {state_footprints_sub} f\n",
    "WHERE\n",
    "    c.geoid = '{geoid}' AND\n",
    "    ST_Intersects(c.geom, f.centroid_geom);\"\"\".format(county_tname=ctable_name, state_footprints_sub=subtable_name, geoid=the_geoid)\n",
    "        \n",
    "            #print(q1)\n",
    "\n",
    "            try:\n",
    "                cur = conn.cursor()\n",
    "                print('about to execute query')\n",
    "                cur.execute(q1)\n",
    "                print('about to commit query results')\n",
    "                conn.commit()\n",
    "                # close the database communication\n",
    "                cur.close()\n",
    "\n",
    "            except psycopg2.DatabaseError as error:\n",
    "                print(\"PROBLEM executing the query....\")\n",
    "                print(error)\n",
    "                proc_errors_df.loc[len(proc_errors_df)]=[scode, the_county, the_geoid, error]\n",
    "             \n",
    "                print(\"Closing the CURSOR\")\n",
    "                cur.close()\n",
    "                print(\"Closing the CONN\")\n",
    "                conn.close()\n",
    "                \n",
    "                try:\n",
    "                    print(\"GONNA RESTART THE CON....\")\n",
    "                    conn = psycopg2.connect(\"dbname='pattyf' user='pattyf' host='localhost' password='XXXXXXX'\")\n",
    "                    print(\"connected\")\n",
    "                except psycopg2.DatabaseError as error:\n",
    "                    print(\"CANT RESTART CONN\")\n",
    "                    print(error)\n",
    "                \n",
    "\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        print(\"Closing DB Connection.\")\n",
    "        conn.close()\n",
    "\n",
    "print(\"Back from Postgis\")\n",
    "#Save processing errors\n",
    "proc_errors_df.to_csv(\"bldg_footprints_proc_errors3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note**: no processing errors were recorded for the above operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15. Output the county tables to shapefiles\n",
    "\n",
    "Create a shell script to dump the PostGIS county tables to CSV files containing the footprint geometry as WKT (well-known text). We do this becuse of shapefile limitations & because the output will be smaller & compress well compared to geojson files.\n",
    "\n",
    "For example:\n",
    "<pre>\n",
    "ogr2ogr -f \"CSV\" -lco GEOMETRY=AS_WKT outfile.csv PG:\"host=localhost dbname=pattyf user=pattyf port=5432\" county_footprints_table\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a scriptfile to ouput all county footprint tables to CSV\n",
    "# CSV smaller than GeoJSON \n",
    "# Shapefiles have size limits & will truncate the data\n",
    "f = open('export_county_footprint_to_csv.sh', 'w')\n",
    "\n",
    "for index, row in sfips_lookup.iterrows():\n",
    "    scode = row['ABBREV'].lower()\n",
    "    sfips = row['FIPS']\n",
    "    subtable_name = scode + \"_footprints_sub\" \n",
    "\n",
    "    print(\"Processing : \", scode, \" With FIPS: \", sfips)\n",
    "    state_county_df = uscounties[uscounties['STATEFP']==sfips]\n",
    "\n",
    "    for item, row in state_county_df.iterrows():\n",
    "        the_geoid = row['GEOID']\n",
    "        the_county = row['NAME']\n",
    "        ctable_name = scode + \"_\" + the_geoid + \"_footprints\"\n",
    "        print(\"About to write cmd for table: \", ctable_name)\n",
    "        out_csvfile = ctable_name + \".csv\"\n",
    "        f.write('ogr2ogr -f \"CSV\" -lco GEOMETRY=AS_WKT ' + out_csvfile + ' PG:\"host=localhost dbname=pattyf user=pattyf port=5432\" ' + ctable_name + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16. Run the script to create county shapefiles\n",
    "\n",
    "Move this script into an empty directory first!\n",
    "\n",
    "Make the script executeable (chmod 755 export_county_footprint_to_csv.sh).\n",
    "\n",
    "Run the script (./export_county_footprint_to_csv.sh) \n",
    "\n",
    "Wait for it to finish - takes about an hour for all counties for all states.\n",
    "\n",
    "Check the results...\n",
    "\n",
    "- Is there a shp file for each county in the 50 states plus DC?\n",
    "\n",
    "- Do the number of features in the county tables add up to the total in the states tables and in the table on the MS github website (125,192,184)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17. Check the Results\n",
    "\n",
    "Do we have one footprint file/PostGIS table per county? \n",
    "\n",
    "- If not, why not? Only reason should be because no features in that county (unlikely)?\n",
    "\n",
    "Do the number of buildings listed on the Microsoft USBuildingFootprints github page match the number of building footprints in the output csv files (and PostGIS tables)? If yes done!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many building footprints in total are in the MS BuildingFootprints files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ms_website_total = 125192184\n",
    "msbld_count= sum(sfips_lookup['BLDG_COUNT'])\n",
    "msbld_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many footprints are in the output county files?\n",
    "\n",
    "We can use **wc -l *.csv > county_feature_counts.csv** to create a file of counts and counties that we can read into a pandas df. Note we did a little post-processing of line_count.csv to remove leading zeros, not county csv files, and add headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in county feature counts\n",
    "fcounts_df =  pd.read_csv(\"county_feature_counts.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcounts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_count=sum(fcounts_df['count'])  # should be 125192184\n",
    "print(\"Number of footprints in the county ouput files: \", f_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference in the total MS Building count & the number of footprints in the output county files?\n",
    "\n",
    "- What amount of difference is acceptable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_diff = msbld_count - f_count\n",
    "if the_diff > 0:\n",
    "    print(\"There are \", the_diff, \"FEWER footprints in the county ouput files.\")\n",
    "elif the_diff < 0:\n",
    "     print(\"There are \", the_diff, \"MORE footprints in the county ouput files.\")\n",
    "else:\n",
    "    print(\"There are the same number of footprints in the county output files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How Many counties were processed?\n",
    "processed_counties = len(fcounts_df.index)\n",
    "print(\"The number of US Counties processed: \", processed_counties )\n",
    "\n",
    "# What is the number of counties in the US States plus DC?\n",
    "# American Samoa (60), Guam (66), Northern Mariana Islands(69),\n",
    "# Puerto Rico (72), Virgin Islands of the U.S. (78)\n",
    "territories = ['60', '66','69','72','78']\n",
    "uscounties_51 = uscounties[~uscounties['STATEFP'].isin(territories)]\n",
    "total_counties51 = len(uscounties_51.index)\n",
    "print(\"Total number of counties in the 50 US states plus DC: \", total_counties51)\n",
    "\n",
    "if total_counties51 == processed_counties:\n",
    "    print(\"The number of counties processed and output to file equals the total number of US State counties plus DC.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table summarizing the number of footprints output per state.\n",
    "\n",
    "This will summarize the discrepancies between the input state footprints files and the output county files. It's up to the individual user to assess whether any differences are acceptable for the application at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add GEOID to fcounts_df\n",
    "#add state fips to fcounts_df\n",
    "#add fcounts to sfips_lookup.df\n",
    "# see what states have big mismatches to identify scope of probelm\n",
    "# also figure out why some counties have zero features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcounts_df['state'] = fcounts_df['countyfile'].str.split('_').str[0]\n",
    "#fcounts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcounts2 =fcounts_df.groupby(['state']).sum()\n",
    "fcounts2.reset_index(level=0, inplace=True)\n",
    "fcounts2['state'] = fcounts2['state'].str.upper()\n",
    "fcounts2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(fcounts2.index) == len(sfips_lookup.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the files\n",
    "sfips2 =sfips_lookup.merge(fcounts2,left_on='ABBREV', right_on='state', how='outer')\n",
    "#sfips2  # This is our summary stats for ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check the table totals\n",
    "print( \"Total foootprint count: \", sum(sfips2['BLDG_COUNT']))\n",
    "print(\" County footprint count: \", sum(sfips2['count']))\n",
    "print(\"Count differences: \", sum(sfips2['BLDG_COUNT']) - sum(sfips2['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the differences to the table\n",
    "sfips2['count_diff'] = sfips2['BLDG_COUNT'] - sfips2['count']\n",
    "sfips2.sort_values(by=['count_diff'], inplace=True,  ascending=False)\n",
    "sfips2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write processing metadata to file\n",
    "sfips2.to_csv(\"Output_counts_by_county_and_state.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now zip all the csv files!\n",
    "\n",
    "<pre>\n",
    "for file in *.csv; do zip ${file%.*}.zip $file; done\n",
    "</pre>\n",
    "\n",
    "Hey did you know that these zipped files can be read directly into QGIS?\n",
    "\n",
    "### CRS for all files\n",
    "\n",
    "The CRS (coordinate reference system) or projection or SRS for all output files is EPSG: 4326 or WGS84.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17. Cleanup\n",
    "\n",
    "Delete all files you don't need!\n",
    "\n",
    "Share output so folks don't need to repeat this wheel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done\n",
    "\n",
    "You can access the output files from this processing for the state of California [here](https://drive.google.com/drive/folders/1-XGvS25tQKKQ3HTqWjAfLJ4PaeXJ9yyY?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Last updated December 20, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
